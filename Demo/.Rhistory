print(f"No valid encoder params loaded, direct generate esm representations")
conf = {
'input_embeddings_path' : tmp_file,
'output_file' : output_file,
}
generate_ESM_feature(**conf)
supp_feat_dim = 400
model_params_path = "../pretrained_model/Demo_clef_dpc_pssm_encoder.pt"
model_params_dict = {
tag: [model_params_path, {"feature_dim":supp_feat_dim}]
}
for tag, params in model_params_dict.items():
break
tmp_file
encoder_path
encoder_path, encoder_config = params
encoder_path
esm_config['maxlen']
encoder = clef(**encoder_config)
tmp_output= output_file
loader_config = {'batch_size':64, 'max_num_padding':esm_config['maxlen']}
config = {
'input_file':tmp_file,
'output_file':tmp_output,
'model':encoder,
'params_path':encoder_path,
'loader_config':loader_config
}
generate_clef_feature(**config)
print(f"Generated protein clef representations/embeddings saved at {output_file}")
esm_config = {'pretrained_model_params':None}
esm_config = esm_config if isinstance(esm_config, dict) else {'Final_pool':False, 'maxlen':256}
esm_config['input_fasta'] = input_file
esm_config['output_file'] = tmp_file
esm_config['Return'] = False
if 'pretrained_model_params' not in esm_config:
esm_config['pretrained_model_params'] = os.path.join(find_root_path(), "../pretrained_model/esm2_t33_650M_UR50D.pt")
fasta_to_EsmRep(**esm_config)
shutil.rmtree(tmp_dir)
import shutil
shutil.rmtree(tmp_dir)
input_file
output_file
model_params_dict
tmp_dir
embedding_generator
embedding_generator = fasta_to_EsmRep
esm_config
esm_config = None
generate_protein_representation(input_file,
output_file,
model_params_dict = None,
tmp_dir = "./tmp",
embedding_generator = fasta_to_EsmRep,
esm_config = None
)
def generate_protein_representation(input_file,
output_file,
model_params_dict = None,
tmp_dir = "./tmp",
embedding_generator = fasta_to_EsmRep,
esm_config = None
):
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
if is_fasta_file(input_file):
print(f"Transform representation from fasta file {input_file}")
import uuid
tmp_file = str(uuid.uuid4())+'_tmp_esm'
tmp_file = os.path.join(tmp_dir, tmp_file)
esm_config = esm_config if isinstance(esm_config, dict) else {'Final_pool':False, 'maxlen':256}
esm_config['input_fasta'] = input_file
esm_config['output_file'] = tmp_file
esm_config['Return'] = False
if 'pretrained_model_params' not in esm_config:
esm_config['pretrained_model_params'] = os.path.join(find_root_path(), "../pretrained_model/esm2_t33_650M_UR50D.pt")
try:
fasta_to_EsmRep(**esm_config)
except:
print("Failed to transform fasta into ESM embeddings, make sure esm config is correct")
output_dict = {}
try:
for tag, params in model_params_dict.items():
encoder_path, encoder_config = params
encoder = clef(**encoder_config)
tmp_output= output_file
loader_config = {'batch_size':64, 'max_num_padding':esm_config['maxlen']}
config = {
'input_file':tmp_file,
'output_file':tmp_output,
'model':encoder,
'params_path':encoder_path,
'loader_config':loader_config
}
generate_clef_feature(**config)
except:
print(f"No valid encoder params loaded, direct generate esm representations")
conf = {
'input_embeddings_path' : tmp_file,
'output_file' : output_file,
}
generate_ESM_feature(**conf)
print(f"ESM2 (protein) array saved as {output_file}")
print(f"Done..")
import shutil
try:
shutil.rmtree(tmp_dir)
except:
print(f"Failed to remove temp file in {tmp_dir}.")
generate_protein_representation(input_file,
output_file,
model_params_dict = None,
tmp_dir = "./tmp",
embedding_generator = fasta_to_EsmRep,
esm_config = None
)
embedding_generator
model_params_dict
for tag, params in model_params_dict.items():
encoder_path, encoder_config = params
encoder = clef(**encoder_config)
tmp_output= output_file
loader_config = {'batch_size':64, 'max_num_padding':esm_config['maxlen']}
config = {
'input_file':tmp_file,
'output_file':tmp_output,
'model':encoder,
'params_path':encoder_path,
'loader_config':loader_config
}
generate_clef_feature(**config)
model_params_dict
encoder_path, encoder_config = params
encoder = clef(**encoder_config)
tmp_output= output_file
esm_config
esm_config['maxlen']
reticulate::repl_python()
from Demo_utils import generate_protein_representation
from sys import argv
from argparse import ArgumentParser
mode = 'clef'
esm_model_path = None
model_params_path = "../pretrained_model/Demo_clef_dpc_pssm_encoder.pt"
tag = "exp"
input_file = "./Test_demo.faa"
output_file = "./Test_demo_clef"
supp_feat_dim = 400
model_params_dict = {
tag: [model_params_path, {"feature_dim":supp_feat_dim}]
}
esm_config = {'pretrained_model_params':esm_model_path} if esm_model_path else None
esm_config
embedding_generator
tmp_dir
model_params_dict
output_file
generate_protein_representation(input_file,
output_file,
model_params_dict = None,
tmp_dir = "./tmp",
embedding_generator = fasta_to_EsmRep,
esm_config = None
)
config = {
'input_file':input_file,
'output_file':output_file,
'model_params_dict':model_params_dict,
'esm_config':esm_config
}
from Demo_utils import generate_protein_representation
from sys import argv
from argparse import ArgumentParser
generate_protein_representation(**config)
model_params_dict
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
tmp_dir = "./tmp"
import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import sys
def find_root_path():
try:
current_dir = os.path.dirname(os.path.abspath(__file__))
except:
current_dir = os.getcwd()
project_root = os.path.abspath(os.path.join(current_dir, os.pardir))
return project_root
src_path = os.path.join(os.path.join(find_root_path(), "src"))
if src_path not in sys.path:
sys.path.insert(0, src_path)
from Data_utils import Potein_rep_datasets
from utils import *
from Feature_transform import *
from CLEF import clef
from Module import test_dnn
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
model_params_dict
for tag, params in model_params_dict.items():
break
encoder_path, encoder_config = params
encoder_path
encoder = clef(**encoder_config)
clef
encoder
encoder.load_state_dict(torch.load('../pretrained_model/Demo_clef_dpc_pssm_encoder.pt'))
torch.load('../pretrained_model/Demo_clef_dpc_pssm_encoder.pt')
loaded_parans = torch.load(params_path, map_location=device)
torch.load('../pretrained_model/Demo_clef_dpc_pssm_encoder.pt', map_location='cpu')
reticulate::repl_python()
from Demo_utils import generate_protein_representation
from sys import argv
from argparse import ArgumentParser
mode = 'clef'
esm_model_path = None
model_params_path = "../pretrained_model/Demo_clef_dpc_pssm_encoder.pt"
tag = "exp"
input_file = "./Test_demo.faa"
output_file = "./Test_demo_clef"
supp_feat_dim = 400
model_params_dict = {
tag: [model_params_path, {"feature_dim":supp_feat_dim}]
}
esm_config = {'pretrained_model_params':esm_model_path} if esm_model_path else None
config = {
'input_file':input_file,
'output_file':output_file,
'model_params_dict':model_params_dict,
'esm_config':esm_config
}
generate_protein_representation(**config)
parser = ArgumentParser(description="Transform protein sequence [fasta file] into encoded representation [arrays].")
input_file = "./Test_demo_clef"
output = "./Test_demo_T6SE_resut.xlsx"
classifier_config = {'num_embeds':1280, "out_dim":1}
rep_path = input_file
input_file = "./Test_demo_clef"
output_file = "./Test_demo_T6SE_resut.xlsx"
rep_path = input_file
classifier_config = {'num_embeds':1280, "out_dim":1}
classifier_path = "../pretrained_model/clef_dpcpssm_T6_classifier.pt"
classifier_path = "../pretrained_model/clef_dpcpssm_T6_classifier.pt"
classifer = test_dnn(**classifier_config)
from Module import test_dnn
classifer = test_dnn(**classifier_config)
classifier_path
config = {
'rep_path':rep_path,
'output_file':None,
'model':classifer,
'params_path':classifier_path,
'Return':True
}
tmp_preds = predict_from_1D_rep(**config)
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Load model weights from {params_path}")
try:
model.load_state_dict(torch.load(params_path))
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
tmp_preds = predict_from_1D_rep(**config)
from Data_utils import Potein_rep_datasets
from utils import *
from Feature_transform import *
from CLEF import clef
from Module import test_dnn
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Load model weights from {params_path}")
try:
model.load_state_dict(torch.load(params_path))
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
tmp_preds = predict_from_1D_rep(**config)
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Load model weights from {params_path}")
try:
loaded_params = torch.load(params_path, map_location=device)
model.load_state_dict(loaded_params)
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
rep_path = input_file
classifier_config = {'num_embeds':1280, "out_dim":1}
classifier_path = args.weight
classifer = test_dnn(**classifier_config)
config = {
'rep_path':rep_path,
'output_file':None,
'model':classifer,
'params_path':classifier_path,
'Return':True
}
tmp_preds = predict_from_1D_rep(**config)
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Loading model weights from {params_path}")
try:
loaded_params = torch.load(params_path, map_location=device)
model.load_state_dict(loaded_params)
print(f"Load classifier weights successfully")
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
rep_path = input_file
classifier_config = {'num_embeds':1280, "out_dim":1}
classifier_path = args.weight
classifer = test_dnn(**classifier_config)
config = {
'rep_path':rep_path,
'output_file':None,
'model':classifer,
'params_path':classifier_path,
'Return':True
}
tmp_preds = predict_from_1D_rep(**config)
if len(output_dict) == 0:
output_dict = tmp_preds
output_dict = tmp_preds
tmp_preds
len(output_dict.columns)
output_dict.columns = [x if i <len(output_dict.columns) - 1 else f'score_{tag}' for i,x in enumerate(output_dict.columns)]
output_dict
print(output_dict.iloc[:10,:])
print(f"Predictions saved at {output_file}")
output_dict.to_excel(output_file, index = False)
print(f'''=====
{output_dict.iloc[:10,:]}
''')
print(f'''
{output_dict.iloc[:10,:]}
''')
print(output_dict.iloc[:10,:])
output_dict.columns = [x if i <len(output_dict.columns) - 1 else 'score' for i,x in enumerate(output_dict.columns)]
print(output_dict.iloc[:10,:])
cutoff = 0.5
output_dict['Predict'] = ['Yes' if x >= cutoff else 'No' output_dict['score']]
['Yes' if x >= cutoff else 'No' for x in output_dict['score']]
output_dict['Predict'] = ['Yes' if x >= cutoff else 'No' for x in output_dict['score']]
print(output_dict.iloc[:10,:])
import torch
import esm
import Bio
import einops
import numpy
import sklearn
import pandas
import umap
print("torch:", torch.__version__)
print("fair-esm:", esm.__version__)
print("biopython:", Bio.__version__)
print("einops:", einops.__version__)
print("numpy:", numpy.__version__)
print("scikit-learn:", sklearn.__version__)
print("pandas:", pandas.__version__)
print("umap-learn:", umap.__version__)
