esm_config = None
)
config = {
'input_file':input_file,
'output_file':output_file,
'model_params_dict':model_params_dict,
'esm_config':esm_config
}
from Demo_utils import generate_protein_representation
from sys import argv
from argparse import ArgumentParser
generate_protein_representation(**config)
model_params_dict
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
tmp_dir = "./tmp"
import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import sys
def find_root_path():
try:
current_dir = os.path.dirname(os.path.abspath(__file__))
except:
current_dir = os.getcwd()
project_root = os.path.abspath(os.path.join(current_dir, os.pardir))
return project_root
src_path = os.path.join(os.path.join(find_root_path(), "src"))
if src_path not in sys.path:
sys.path.insert(0, src_path)
from Data_utils import Potein_rep_datasets
from utils import *
from Feature_transform import *
from CLEF import clef
from Module import test_dnn
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
model_params_dict
for tag, params in model_params_dict.items():
break
encoder_path, encoder_config = params
encoder_path
encoder = clef(**encoder_config)
clef
encoder
encoder.load_state_dict(torch.load('../pretrained_model/Demo_clef_dpc_pssm_encoder.pt'))
torch.load('../pretrained_model/Demo_clef_dpc_pssm_encoder.pt')
loaded_parans = torch.load(params_path, map_location=device)
torch.load('../pretrained_model/Demo_clef_dpc_pssm_encoder.pt', map_location='cpu')
reticulate::repl_python()
from Demo_utils import generate_protein_representation
from sys import argv
from argparse import ArgumentParser
mode = 'clef'
esm_model_path = None
model_params_path = "../pretrained_model/Demo_clef_dpc_pssm_encoder.pt"
tag = "exp"
input_file = "./Test_demo.faa"
output_file = "./Test_demo_clef"
supp_feat_dim = 400
model_params_dict = {
tag: [model_params_path, {"feature_dim":supp_feat_dim}]
}
esm_config = {'pretrained_model_params':esm_model_path} if esm_model_path else None
config = {
'input_file':input_file,
'output_file':output_file,
'model_params_dict':model_params_dict,
'esm_config':esm_config
}
generate_protein_representation(**config)
parser = ArgumentParser(description="Transform protein sequence [fasta file] into encoded representation [arrays].")
input_file = "./Test_demo_clef"
output = "./Test_demo_T6SE_resut.xlsx"
classifier_config = {'num_embeds':1280, "out_dim":1}
rep_path = input_file
input_file = "./Test_demo_clef"
output_file = "./Test_demo_T6SE_resut.xlsx"
rep_path = input_file
classifier_config = {'num_embeds':1280, "out_dim":1}
classifier_path = "../pretrained_model/clef_dpcpssm_T6_classifier.pt"
classifier_path = "../pretrained_model/clef_dpcpssm_T6_classifier.pt"
classifer = test_dnn(**classifier_config)
from Module import test_dnn
classifer = test_dnn(**classifier_config)
classifier_path
config = {
'rep_path':rep_path,
'output_file':None,
'model':classifer,
'params_path':classifier_path,
'Return':True
}
tmp_preds = predict_from_1D_rep(**config)
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Load model weights from {params_path}")
try:
model.load_state_dict(torch.load(params_path))
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
tmp_preds = predict_from_1D_rep(**config)
from Data_utils import Potein_rep_datasets
from utils import *
from Feature_transform import *
from CLEF import clef
from Module import test_dnn
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Load model weights from {params_path}")
try:
model.load_state_dict(torch.load(params_path))
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
tmp_preds = predict_from_1D_rep(**config)
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Load model weights from {params_path}")
try:
loaded_params = torch.load(params_path, map_location=device)
model.load_state_dict(loaded_params)
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
rep_path = input_file
classifier_config = {'num_embeds':1280, "out_dim":1}
classifier_path = args.weight
classifer = test_dnn(**classifier_config)
config = {
'rep_path':rep_path,
'output_file':None,
'model':classifer,
'params_path':classifier_path,
'Return':True
}
tmp_preds = predict_from_1D_rep(**config)
def predict_from_1D_rep(rep_path, model, params_path,
output_file = None,
Return = True):
predictset = Potein_rep_datasets({'feature':rep_path})
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model.to(device)
if isinstance(model, torch.nn.Module):
model.eval()
if params_path:
print(f"Loading model weights from {params_path}")
try:
loaded_params = torch.load(params_path, map_location=device)
model.load_state_dict(loaded_params)
print(f"Load classifier weights successfully")
except:
print(f"Failed to load model weights from {params_path}")
output_dict = {
'ID':[]
}
tmp_scores = []
for batch in predictset.Dataloader(batch_size=64, shuffle=False, test=True, max_num_padding=None, device=device):
with torch.no_grad():
y_hat = model(batch)
output_dict['ID'].extend(batch['ID'])
y_hat = y_hat.detach().to('cpu').numpy()
tmp_scores.append(y_hat)
tmp_scores = np.concatenate(tmp_scores, 0)
if len(tmp_scores.shape) > 1:
for i in range(tmp_scores.shape[1]):
tag = f"score_{i}"
output_dict[tag] = tmp_scores[:,i]
else:
output_dict['score'] = tmp_scores
output_dict = pd.DataFrame(output_dict)
if output_file:
try:
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
except:
print(f'Predictions failed to save as {output_file}')
import uuid
tmp_name = str(uuid.uuid4())+'_clef'
output_file =os.path.join(os.path.dirname(input_file), tmp_name)
output_dict.to_excel(output_file)
print(f'Predictions saved as {output_file}')
if Return:
return output_dict
rep_path = input_file
classifier_config = {'num_embeds':1280, "out_dim":1}
classifier_path = args.weight
classifer = test_dnn(**classifier_config)
config = {
'rep_path':rep_path,
'output_file':None,
'model':classifer,
'params_path':classifier_path,
'Return':True
}
tmp_preds = predict_from_1D_rep(**config)
if len(output_dict) == 0:
output_dict = tmp_preds
output_dict = tmp_preds
tmp_preds
len(output_dict.columns)
output_dict.columns = [x if i <len(output_dict.columns) - 1 else f'score_{tag}' for i,x in enumerate(output_dict.columns)]
output_dict
print(output_dict.iloc[:10,:])
print(f"Predictions saved at {output_file}")
output_dict.to_excel(output_file, index = False)
print(f'''=====
{output_dict.iloc[:10,:]}
''')
print(f'''
{output_dict.iloc[:10,:]}
''')
print(output_dict.iloc[:10,:])
output_dict.columns = [x if i <len(output_dict.columns) - 1 else 'score' for i,x in enumerate(output_dict.columns)]
print(output_dict.iloc[:10,:])
cutoff = 0.5
output_dict['Predict'] = ['Yes' if x >= cutoff else 'No' output_dict['score']]
['Yes' if x >= cutoff else 'No' for x in output_dict['score']]
output_dict['Predict'] = ['Yes' if x >= cutoff else 'No' for x in output_dict['score']]
print(output_dict.iloc[:10,:])
import torch
import esm
import Bio
import einops
import numpy
import sklearn
import pandas
import umap
print("torch:", torch.__version__)
print("fair-esm:", esm.__version__)
print("biopython:", Bio.__version__)
print("einops:", einops.__version__)
print("numpy:", numpy.__version__)
print("scikit-learn:", sklearn.__version__)
print("pandas:", pandas.__version__)
print("umap-learn:", umap.__version__)
reticulate::repl_python()
import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import sys
reticulate::repl_python()
import os
import pandas as pd
import sys
from Demo_utils import predict_from_1D_rep
from argparse import ArgumentParser
from Module import test_dnn
reticulate::repl_python()
import os
import pandas as pd
import sys
from Demo_utils import predict_from_1D_rep
from argparse import ArgumentParser
from Module import test_dnn
def find_root_path():
try:
current_dir = os.path.dirname(os.path.abspath(__file__))
except:
current_dir = os.getcwd()
project_root = os.path.abspath(os.path.join(current_dir, os.pardir))
return project_root
root_path = find_root_path()
root_path
pretrained_model_params = pretrained_model_params if pretrained_model_params else os.path.join(root_path, './pretained_model/esm2_t33_650M_UR50D.pt')
pretrained_model_params = None
pretrained_model_params = pretrained_model_params if pretrained_model_params else os.path.join(root_path, './pretained_model/esm2_t33_650M_UR50D.pt')
pretrained_model_params
tmp_dir = "./tmp"
model_initial = clef
import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import sys
def find_root_path():
try:
current_dir = os.path.dirname(os.path.abspath(__file__))
except:
current_dir = os.getcwd()
project_root = os.path.abspath(os.path.join(current_dir, os.pardir))
return project_root
src_path = os.path.join(os.path.join(find_root_path(), "src"))
if src_path not in sys.path:
sys.path.insert(0, src_path)
from Data_utils import Potein_rep_datasets
from utils import *
from Feature_transform import *
from CLEF import clef
from Module import test_dnn
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
tmp_dir = "./tmp"
model_initial = clef
embedding_generator = fasta_to_EsmRep
esm_config = None
train_config = None
if not os.path.exists(tmp_dir):
os.mkdir(tmp_dir)
input_file_config = {'fasta':"../Data/Dataset S1/Pretrain-6965.faa", "supp_feat":"./Demo_train_feat"}
assert "fasta" in  input_file_config and "supp_feat" in input_file_config, "PATH for training .fasta file and feature file are needed"
input_file = input_file_config['fasta']
assert is_fasta_file(input_file)
print("\033[91m这是一个错误提示信息\033[0m")
f"\033[91m{input_file} is not a fasta format file\033[0m"
"\033[91m这是一个错误提示信息\033[0m"
input_file = None
assert is_fasta_file(input_file), f"\033[91m{input_file} is not a fasta format file\033[0m"
input_file = input_file_config['fasta']
assert is_fasta_file(input_file), f"\033[91m{input_file} is not a fasta format file\033[0m"
input_feat = input_file_config['supp_feat']
path = "Demo_train_feat"
x = load_feature_from_local(path)
tmp = value.shape
for key, value in x.items():
break
value.shape
tmp = None
for key, value in x.items():
if not tmp:
tmp = value.shape
elif tmp != value.shape:
tmp = None
break
else:
tmp = value.shape
tmp
def get_feature_dim(path):
x = load_feature_from_local(path)
tmp = None
for key, value in x.items():
if not tmp:
tmp = value.shape
elif tmp != value.shape:
tmp = None
break
else:
tmp = value.shape
return tmp
get_feature_dim(path)
assert get_feature_dim(input_feat)
assert get_feature_dim(input_feat), f"{input_feat} is not a dict containing feature arrays"
print(f"Transform representation from fasta file {input_file}")
import uuid
tmp_file = str(uuid.uuid4())+'_tmp_esm'
tmp_file = os.path.join(tmp_dir, tmp_file)
esm_config = esm_config if isinstance(esm_config, dict) else {'Final_pool':False, 'maxlen':256, 'Return':False}
esm_config['input_fasta'] = input_file
esm_config['output_file'] = tmp_file
esm_config['Return'] = False
print("\x1b[91mFailed to transform fasta into ESM embeddings, make sure esm config is correct\x1b[0m")
raise TypeError("Failed to transform fasta into ESM embeddings, make sure esm config is correct")
import torch.optim as optim
import random
import pandas as pd
from Data_utils import Potein_rep_datasets
from Module import ContrastiveLoss
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model_initial = clef
{'num_embeds':1280,"feature_dim":get_feature_dim(input_feat)[-1]}
model_config = model_config if model_config else {'num_embeds':1280,"feature_dim":get_feature_dim(input_feat)[-1]}
model_config = None
model_config = model_config if model_config else {'num_embeds':1280,"feature_dim":get_feature_dim(input_feat)[-1]}
model = model_initial(**model_config).to(device)
tmp_file
data_path_config = {'esm_feature':tmp_file, "B_feature":input_feat}
Dataset = Potein_rep_datasets(data_path_config)
raise ValueError("Failed to load feature")
if len(Dataset) == 0:
raise ValueError("Failed to load featurea for training")
loss_function= ContrastiveLoss()
for x in ('lr', 'batch_size', 'num_epoch')
assert x in  train_config
for x in ('lr', 'batch_size', 'num_epoch'):
assert x in  train_config
('lr', 'batch_size', 'num_epoch')
for x in ['lr', 'batch_size', 'num_epoch']:
assert x in  train_config
train_config
train_config = [1]
assert x in  train_config, ""
assert x in  train_config, "'lr', 'batch_size' and 'num_epoch' are needed when not using default train configuration "
optimizer =optim.Adam(model.parameters(), lr=train_config['lr'])
train_config = None
train_config = {'lr':0.0002, 'batch_size':128, 'num_epoch':15}
for x in ['lr', 'batch_size', 'num_epoch']:
assert x in  train_config, "'lr', 'batch_size' and 'num_epoch' are needed when not using default train configuration "
optimizer =optim.Adam(model.parameters(), lr=train_config['lr'])
esm_config['maxlen']
train_config['lr']
num_epoch
lr = 0.0002
batch_size = 128
num_epoch = 20
T = trainer(train_epoch = train_epoch_clef)
