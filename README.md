# CLEF
Contrastive Learning of language Embedding and biological Feature is a contrastive learning framework used to combine information from supplemental biological features or experimental data with protein language model [ESM2](https://github.com/facebookresearch/esm) generated embeddings. Generated cross-modal feature can be used for better downstream protein prediction like Gram-negative bacterial effectors prediction or virulence factors discovery.


![](./Material/Main.jpg)

## Set up

### Requirement
The project is implemented with python (3.11.3), the following library packages are required:

```txt
torch==2.0.0
fair-esm==2.0.0
biopython==1.83
einops==0.8.0
numpy==1.24.2
scikit-learn==1.2.2
pandas==2.0.3
```
We also tested the code with recent versions of these requirements, other reasonable versions should work as well.
The code was tested on Windows and Ubuntu.

### Installation

Required Python packages are listed in `requirements.txt` file.

To install the required packages, run the following command using pip:
```shell
pip install -r requirements.txt
```


## Demo

CLEF was trained under a contrative learning framework, and can generate cross-modal representations based on pre-trained protein language models (pLMs) of [ESM2](https://github.com/facebookresearch/esm)
The generated cross-modal representations can be used in other downstream predictions task and enhance the protein classification performance.

### Download weights
We provide several pre-trained model weights in the `./pretrained_model` directory to enable users to test the demo code provided below.

 **Note**: some weight files exceed 100 MB and are uploaded using Git LFS. If you clone all the repository directly, you may encounter errors when loading these weights. 

To address this, we recommend downloading these files manually to your local environment from page `./pretrained_model`.

**Or** accessing the weights via [Google Drive](https://drive.google.com/drive/u/1/folders/1OAmn487vu3e4J258eMhcX8vTuZzcIX0d).

### Generate Cross-Modal Representation

We provide some example code in `Demo` to use the CLEF, cross-modal representations based on ESM2 can be generated by running `GenerateCrossModalityRep.py`:
```shell
cd .Demo
python GenerateCrossModalityRep.py --In Test_demo.faa --Out Test_clef_rep --weight ../pretrained_model/CLEF-DP+MSA+3Di+AT.pt 
```
 Parameters:

- `--In` fasta file of input proteins.
- `--Out` output protein representation arrays file.
- `--weight` pretrained CLEF model parameters path, here we use the example model `CLEF-DP+MSA+3Di+AT.pt ` in  `./pretrained_model`


**Note**: You need to download the pretrained model weights locally to use ESM for protein sequence encoding. You can manually download the ESM2-650M [model](https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt) and [regression](https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt). Alternatively, `fair-esm` will automatically download the model to `./pretrained_model/checkpoints/`.

This will create a file `Test_clef_rep`, containing the cross-modal representations of input proteins

 ### Predict using the Protein Representation

After that, the generated representations can be used in other protein classification tasks, here we give an example of T6SE prediction:

```shell
python PredictProteinClassification.py --In Test_clef_rep --Out Test_result.csv --weight ../pretrained_model/T6classifier-CLEF-DP+MSA+3Di+AT-0.5cutoff.pt 
```
Parameters:

- `--In` file of input proteins representation, `Test_clef_rep` is a dict looks like {sample_id:1D numpy array}.
- `--Out` output prediction result CSV table.
- `--weight` classifier weights path, `T6classifier-CLEF-DP+MSA+3Di+AT-0.5cutoff.pt` is a simple multilayer perceptron (MLP) trained to discriminate T6SE and non-T6SE

The prediction results will be listed in an CSV table `Test_result.csv`.


### Train a CLEF model 

This framework provides a method to perform cross-modal training between embeddings from language model and other related-modality features like structure and annotations

To train a CLEF encoder, you need to prepare `fasta` format files for the protein samples (or encoded 2D-representations) and the corresponding feature files for those proteins. 

Training with different features will yield different CLEF models, generating distinct cross-modal representations that may perform differently in various downstream tasks. 

In our provided demo code, you can input multiple feature vectors of any length (a dict looks like {sample_id:1D numpy array}) for training, the demo data is provided at `./Demo/Demo_train/`:

```shell
python CLEFTrain.py --Seq ./Demo_train/Demo_trainset.faa --Feat ./Demo_train/Demo_trainset_featA ./Demo_train/Demo_trainset_featB ./Demo_train/Demo_trainset_featC --Out Demo_clef --lr 0.00002 --epoch 20 --btz 128
```
Parameters:

- `--Seq` file of input trainset proteins sequences, can be `.fasta` file or encoded embeddings dict by PLM
- `--Feat` feature array files of input trainset proteins, you can input multiple feature files for multimodal training. Here we use three different modal features as an example.
- `--Out` output dir containing the training checkpoint and log file.
- `--lr` learning rate for training.
- `--btz` batch size for training for training.
- `--epoch` number of training epoch.
- `--maxlen` max length of input protein sequence that model process, 256 by default.

The training log text files and the model weights checkpoint will be saved in the output directory.

Training the model can be time-consuming and requires adequate hardware support. For example, using an RTX 3060Ti 8GB GPU, training on a dataset of approximately 3,000 proteins for 20 epochs takes around 15 minutes.

## Contact

Please contact Yue Peng at [756028108@qq.com] for questions.


