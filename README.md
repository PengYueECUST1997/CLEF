# CLEF
Contrastive Learning of language Embedding and biological Feature is a contrastive learning framework used to combine information from supplemental biological features or experimental data with protein language model [ESM2](https://github.com/facebookresearch/esm) generated embeddings. Generated cross-modal feature can be used for better downstream protein prediction like Gram-negative bacterial effectors prediction or virulence factors discovery.


![](./Material/Main.jpg)

## Set up

### Requirement
The project is implemented with python (3.11.3), the following library packages are required:

```txt
torch==2.0.1
fair-esm==2.0.0
biopython==1.79
einops==0.7.0
numpy==1.23.4
scikit-learn==1.1.3
```
We also tested the code with recent versions of these requirements, other reasonable versions should work as well.
The code was tested on Windows.

### Installation

Required Python packages are listed in `requirements.txt` file.

To install the required packages, run the following command using pip:
```shell
pip install -r requirements.txt
```

To output the result table, the package `pandas` was used in code: 
```shell
pip install pandas 
```

## Demo

CLEF was trained under a contrative learning framework, and can generate cross-modal representations based on pre-trained protein language models (pLMs) of [ESM2](https://github.com/facebookresearch/esm)
The generated cross-modal representations can be used in other downstream predictions task and enhance the protein classification performance.


### Generate Cross-Modal Representation

We provide some example code in `Demo` to use the CLEF, cross-modal representations based on ESM2 can be generated by running `GenerateCrossModalityRep.py`:
```shell
cd .Demo
python GenerateCrossModalityRep.py --In Test_demo.faa --Out Test_demo_clef --weight ..\pretrained_model\Demo_clef_dpc_pssm_encoder.pt --supp_feat_dim 400 
```
 Parameters:

- `--In` fasta file of input proteins.
- `--Out` output protein representation arrays file.
- `--weight` pretrained CLEF model parameters path, here we use the example model `Demo_clef_dpc_pssm_encoder.pt` trained by DPC-PSSM feature in  `pretrained_model`
- `--supp_feat_dim` numbers of dimensions of biological features used for CLEF (need to match with pretrained CLEF model parameters and here is 400)

**Note**: You need to download the pretrained model weights locally to use ESM for protein sequence encoding. You can manually download the ESM2-650M [model](https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt) and [regression](https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt). Alternatively, `fair-esm` will automatically download the model to `C:\Users\xxx\.cache\torch\hub\checkpoints\`.

This will create a file `Test_demo_clef`, containing the cross-modal representations of input proteins

 ### Predict using the Protein Representation

After that, the generated representations can be used in other protein classification tasks, here we give an example of T6SE prediction:

```shell
python PredictProteinClassification.py --In Test_demo_clef --Out Test_result.xlsx --weight ..\pretrained_model\clef_dpcpssm_T6_classifier.pt
```
Parameters:

- `--In` fasta file of input proteins.
- `--Out` output prediction result Excel table.
- `--weight` classifier weights path, `clef_dpcpssm_T6_classifier.pt` is a simple multilayer perceptron (MLP) trained to discriminate T6SE and non-T6SE

The prediction results will be listed in an Excel table `Test_result.xlsx`.

### Convert biological information into feature 

If you want to experiment with incorporating different protein-related biological features into CLEF and train a new cross-modal representation encoder, you first need to encode the corresponding biological information into feature arrays or tensors. In our paper, we listed some features we experimented with and their extraction methods. Here is an example:


For encoding multiple sequence alignments (MSA), we use the `msa-transformer` pretrained [model](https://dl.fbaipublicfiles.com/fair-esm/models/esm_msa1b_t12_100M_UR50S.pt) ( as well as [regression](https://dl.fbaipublicfiles.com/fair-esm/regression/esm_msa1b_t12_100M_UR50S-contact-regression.pt) is needed ). In the example code below, we convert each MSA in the `"./Demo/Demo_MSA/"` directory in `.fasta` format into a 768-dimensional array:

```python
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method 

config = {
    'input_alignments_dir': "./Demo/Demo_MSA/",  # input dirname of MSA 
    'maxlen': 1024,
    'output_file': "./Demo/Demo_MSA_feat",  # output feature path
    'remapping_fasta': None,
    'clust_pool': True,  # average pooling across the cluster (first) dimension 
    'res_pool': True,  # average pooling across the residue (second) dimension
    'maxmsa': 8,
    'suffix': 'fasta'
}
generate_msa_transformer_feat(**config)
```

Then a file containing the output arrays will be saved at `"./Demo/Demo_MSA_feat",`

### Train a CLEF model 

 To train a CLEF encoder, you need to prepare `.fasta` files for the protein samples and the corresponding feature files for those proteins. Training with different features will yield different CLEF models, generating distinct cross-modal representations that may perform differently in various downstream tasks. In our provided demo code, you can input feature vectors of any length (1D tensors) for training:

```shell
python CLEFTrain.py --Fa Train_demo.faa --Feat Train_demo_feat --Out Demo_clef --lr 0.0002 --btz 128 --epoch 20
```
Parameters:

- `--Fa` fasta file of input trainset proteins.
- `--Feat` feature array files of input trainset proteins.
- `--Out` output dir containing the training checkpoint.
- `--lr` learning rate for training.
- `--btz` batch size for training for training.
- `--epoch` number of training epoch.

The training log files and the model weights `checkpoint.pt` will be saved in the output directory.

Training the model can be time-consuming and requires adequate hardware support. For example, using an RTX 3060Ti 8GB GPU, training on a dataset of approximately 8,000 proteins for 20 epochs takes around 40 minutes.

## Contact

Please contact Yue Peng at [756028108@qq.com] for questions.


