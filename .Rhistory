from Metrics_method import compute_acc, compute_f1, compute_mcc
data_path_config = {'esm_feature':"../Benchmark-partB/DeepSecE_test_esm(512)",
"B_feature":"../Benchmark-partB/BastionX/CLEF_feature/DeepSecE_train_bastionX_ori"}
Dataset = Potein_rep_datasets(data_path_config)
x = pickle.load(open("../Benchmark-partB/DeepSecE_test_esm(512)", "rb"))
y = {}
y = {key.split('~')[0]:value for key, value in x.items()}
len(y)
pickle.dump(y, open("../Benchmark-partB/DeepSecE_test_esm(512)", "wb"))
reticulate::repl_python()
import numpy as np
import pickle
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
import random
import os
import pandas as pd
from Data_utils import Potein_rep_datasets
from Module import test_dnn, BCELoss, ContrastiveLoss
from utils import Metrics, Train
from Metrics_method import compute_acc, compute_f1, compute_mcc
data_path_config = {'esm_feature':"../Benchmark-partB/DeepSecE_test_esm(512)",
"B_feature":"../Benchmark-partB/BastionX/CLEF_feature/DeepSecE_train_bastionX_ori"}
Dataset = Potein_rep_datasets(data_path_config)
x = pickle.load(open("../Benchmark-partB/BastionX/CLEF_feature/DeepSecE_train_bastionX_ori", "rb"))
y = next(iter(x.values()))
y.shape
rom CLEF import clef_3DI
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_3DI(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
optimizer =optim.Adam(model.parameters(), lr=lr)
loss_function= ContrastiveLoss()
batch_size = 128
from CLEF import clef_for_publish
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
optimizer =optim.Adam(model.parameters(), lr=lr)
loss_function= ContrastiveLoss()
batch_size = 128
max_num_padding = 512
monitor_score = None
train_iterator = Dataset.Dataloader
test_iterator = Dataset.Dataloader
monitor = Metrics(metrics_dict = {})
num_epoch = 15
monitor_score = None
from utils import test_clef, train_epoch_clef, trainer
train_config = {
'train_iterator':Dataset.Dataloader,
'loss_function':loss_function,
'optimizer':optimizer,
'batch_size':128,
'max_num_padding':512,
'monitor':monitor,
'device':device
}
test_config = {}
self = trainer(train_epoch = train_epoch_clef)
num_epoch = 5
self.Train(model, num_epoch=num_epoch, train_config=train_config, test_config={},early_stop_patience=30)
X = torch.randn([128, 512, 1280])
X.permute(0, 2, 1).shape
reticulate::repl_python()
import numpy as np
import pickle
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
import random
import os
import pandas as pd
from Data_utils import Potein_rep_datasets
from Module import test_dnn, BCELoss, ContrastiveLoss
from utils import Metrics, Train
from Metrics_method import compute_acc, compute_f1, compute_mcc
data_path_config = {'esm_feature':"../Benchmark-partB/DeepSecE_test_esm(512)",
"B_feature":"../Benchmark-partB/BastionX/CLEF_feature/DeepSecE_train_bastionX_ori"}
Dataset = Potein_rep_datasets(data_path_config)
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
optimizer =optim.Adam(model.parameters(), lr=lr)
loss_function= ContrastiveLoss()
batch_size = 128
max_num_padding = 512
# max_num_padding = None
monitor_score = None
train_iterator = Dataset.Dataloader
test_iterator = Dataset.Dataloader
monitor = Metrics(metrics_dict = {})
num_epoch = 5
monitor_score = None
from utils import test_clef, train_epoch_clef, trainer
train_config = {
'train_iterator':Dataset.Dataloader,
'loss_function':loss_function,
'optimizer':optimizer,
'batch_size':128,
'max_num_padding':512,
'monitor':monitor,
'device':device
}
test_config = {}
self = trainer(train_epoch = train_epoch_clef)
self.Train(model, num_epoch=num_epoch, train_config=train_config, test_config={},early_stop_patience=30)
reticulate::repl_python()
import numpy as np
import pickle
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
import random
import os
import pandas as pd
from Data_utils import Potein_rep_datasets
from Module import test_dnn, BCELoss, ContrastiveLoss
from utils import Metrics, Train
from Metrics_method import compute_acc, compute_f1, compute_mcc
data_path_config = {'esm_feature':"../Benchmark-partB/DeepSecE_test_esm(512)",
"B_feature":"../Benchmark-partB/BastionX/CLEF_feature/DeepSecE_train_bastionX_ori"}
Dataset = Potein_rep_datasets(data_path_config)
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
optimizer =optim.Adam(model.parameters(), lr=lr)
loss_function= ContrastiveLoss()
batch_size = 128
max_num_padding = 512
# max_num_padding = None
monitor_score = None
train_iterator = Dataset.Dataloader
test_iterator = Dataset.Dataloader
monitor = Metrics(metrics_dict = {})
num_epoch = 5
monitor_score = None
from utils import test_clef, train_epoch_clef, trainer
train_config = {
'train_iterator':Dataset.Dataloader,
'loss_function':loss_function,
'optimizer':optimizer,
'batch_size':128,
'max_num_padding':512,
'monitor':monitor,
'device':device
}
test_config = {}
self = trainer(train_epoch = train_epoch_clef)
self.Train(model, num_epoch=num_epoch, train_config=train_config, test_config={},early_stop_patience=30)
monitor = Metrics(metrics_dict = {})
num_epoch = 10
monitor_score = None
self.Train(model, num_epoch=num_epoch, train_config=train_config, test_config={},early_stop_patience=30)
reticulate::repl_python()
from Feature_transform  import generate_clef_feature
input_file = "../Benchmark-partB/DeepSecE_test_esm(512)"
tag = 'bastionX'
dirname = f"./AMP/{tag}"
if not os.path.exists(dirname):
os.mkdir(dirname)
import os
if not os.path.exists(dirname):
os.mkdir(dirname)
header = '_'.join(os.path.split(input_file)[-1].split('_')[:-1])
output_file = os.path.join(dirname, f"{header}_{tag}_clef")
from CLEF import clef_for_publish
model = clef_for_publish(1280, 5)
params_path = "./log/checkpoint_params.pt"
loader_config = {'batch_size':64, 'max_num_padding':512}
params_path = "./log/checkpoint_params.pt"
loader_config = {'batch_size':64, 'max_num_padding':512}
config = {
'input_file':input_file,
'output_file':output_file,
'model':model,
'params_path':params_path,
'loader_config':loader_config
}
generate_clef_feature(**config)
from utils import auto_make_traintest_config, auto_switch_binary_label, assign_labels
input_path = os.path.join(dirname, f"{header}_{tag}_clef")
file = f"{header}_{tag}_clef"
sele = 'T3'
data_type = file.split('_')[-3] if 'esm_rep' not in file else file.split('_')[-4]
data_type
k, l = ('test', 'train') if data_type == 'test' else ('train', 'test')
label_dict = assign_labels((pickle.load(open(input_path, "rb"))).keys(), {sele:1})
import pickle
data_type = file.split('_')[-3] if 'esm_rep' not in file else file.split('_')[-4]
k, l = ('test', 'train') if data_type == 'test' else ('train', 'test')
label_dict = assign_labels((pickle.load(open(input_path, "rb"))).keys(), {sele:1})
pickle.dump(label_dict, open(os.path.join(dirname, f"{header}_{tag}_label"), "wb"))
label_dict = assign_labels((pickle.load(open(input_path.replace(k, l), "rb"))).keys(), {sele:1})
pickle.dump(label_dict, open(os.path.join(dirname, f"{header}_{tag}_label".replace(k, l)), "wb"))
import numpy as np
import pickle
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
import random
import os
import pandas as pd
from Data_utils import Potein_rep_datasets
from Module import test_dnn, BCELoss
from utils import Metrics, Train
from Metrics_method import compute_acc, compute_f1, compute_mcc
import torch.optim as optim
sele = 'T3'
input_path = "./AMP/3Diseq/DeepSecE_test_3Diseq_clef"
import torch.optim as optim
sele = 'T3'
input_path = "./AMP/3Diseq/DeepSecE_test_3Diseq_clef"
tag = os.path.split(input_path)[-1].split('_')[-2]
output_file = f"{tag}_compare_log_{sele}.csv"
initial_model = test_dnn
train_data_config, test_data_config = auto_make_traintest_config(input_path, align = False)
train_data_config
test_data_config = None
auto_switch_binary_label(input_path, sele)
num_trails = 3
feat_dim = 5
monitor = Metrics({'Accuracy':compute_acc, 'F1':compute_f1, "MCC":compute_mcc})
Train_config = {'loss_function':BCELoss(),
'batch_size':32,
'num_epoch':55,
'max_num_padding':None,
'monitor':monitor,
'monitor_score':'Accuracy',
'early_stop_patience':30}
device = 'cuda:0' if torch.cuda.is_available() else "cpu"
test_candidate = [x for x in test_data_config  if x != label_tag]
model_param_config = {'esm':esm_dim, 'clef':256, 'ori':feat_dim, 'concat':feat_dim+esm_dim, 'clefcat':feat_dim+esm_dim}
output = pd.DataFrame({})
lr = 0.000008
model_config = {'num_embeds':1280, 'finial_drop':0.5}
lr = 0.000008
initial_optimizer = optim.Adam
esm_dim = 1280
label_tag = 'label'
train_range = None
test_range = None
device = 'cuda:0' if torch.cuda.is_available() else "cpu"
test_candidate = [x for x in test_data_config  if x != label_tag]
model_param_config = {'esm':esm_dim, 'clef':256, 'ori':feat_dim, 'concat':feat_dim+esm_dim, 'clefcat':feat_dim+esm_dim}
output = pd.DataFrame({})
device = 'cuda:0' if torch.cuda.is_available() else "cpu"
test_candidate = [x for x in test_data_config  if x != label_tag]
label_tag
test_candidate = [x for x in train_data_config  if x != label_tag]
model_param_config = {'esm':esm_dim, 'clef':256, 'ori':feat_dim, 'concat':feat_dim+esm_dim, 'clefcat':feat_dim+esm_dim}
output = pd.DataFrame({})
compare_log = {}
for mode in test_candidate:
tmp_train_config = {'feature':train_data_config[mode], 'label':train_data_config[label_tag]}
Dataset = Potein_rep_datasets(tmp_train_config, label_tag = label_tag)
if train_range:
Dataset.train_range = train_range
if test_data_config:
tmp_test_config = {'feature':test_data_config[mode], 'label':test_data_config[label_tag]}
testset = Potein_rep_datasets(tmp_test_config, label_tag = label_tag)
if test_range:
testset.test_range = test_range
else:
testset = Dataset
model_config['num_embeds'] = model_param_config[mode]
model = initial_model(**model_config).to(device)
optimizer = initial_optimizer(model.parameters(), lr)
Train_config.update({'model':model, 'optimizer':optimizer,
'train_iterator':Dataset.Dataloader,
'test_iterator':testset.Dataloader,
'device':device})
tmp_result = Train(**Train_config)
best_score, train_log, best_epoch = list(tmp_result.values())
tmp_log = {'best_score':best_score}
for key, value in train_log.items():
if key not in  ('Epoch','train_Loss','test_Loss'):
tmp_log[key] = value[best_epoch - 1]
print(f"{mode}:{tmp_log}\n")
compare_log[mode] = tmp_log
mode
train_data_config[mode]
tmp_train_config = {'feature':train_data_config[mode], 'label':train_data_config[label_tag]}
Dataset = Potein_rep_datasets(tmp_train_config, label_tag = label_tag)
test_data_config
testset = Dataset
testset.split_test(0.1)
model_config['num_embeds'] = model_param_config[mode]
model = initial_model(**model_config).to(device)
optimizer = initial_optimizer(model.parameters(), lr)
Train_config.update({'model':model, 'optimizer':optimizer,
'train_iterator':Dataset.Dataloader,
'test_iterator':testset.Dataloader,
'device':device})
tmp_result = Train(**Train_config)
best_score, train_log, best_epoch = list(tmp_result.values())
tmp_log = {'best_score':best_score}
Train_config.update({'model':model, 'optimizer':optimizer,
'train_iterator':Dataset.Dataloader,
'test_iterator':testset.Dataloader,
'device':device})
train_data_config
import torch.optim as optim
sele = 'T3'
input_path = "./AMP/bastionX/DeepSecE_test_bastionX_clef"
tag = os.path.split(input_path)[-1].split('_')[-2]
output_file = f"{tag}_compare_log_{sele}.csv"
initial_model = test_dnn
train_data_config, test_data_config = auto_make_traintest_config(input_path, align = False)
train_data_config
auto_switch_binary_label(input_path, sele)
num_trails = 3
feat_dim = 5
monitor = Metrics({'Accuracy':compute_acc, 'F1':compute_f1, "MCC":compute_mcc})
Train_config = {'loss_function':BCELoss(),
'batch_size':32,
'num_epoch':55,
'max_num_padding':None,
'monitor':monitor,
'monitor_score':'Accuracy',
'early_stop_patience':30}
device = 'cuda:0' if torch.cuda.is_available() else "cpu"
test_candidate = [x for x in train_data_config  if x != label_tag]
model_param_config = {'esm':esm_dim, 'clef':256, 'ori':feat_dim, 'concat':feat_dim+esm_dim, 'clefcat':feat_dim+esm_dim}
output = pd.DataFrame({})
clef_dim = 256
device = 'cuda:0' if torch.cuda.is_available() else "cpu"
test_candidate = [x for x in train_data_config  if x != label_tag]
model_param_config = {'esm':esm_dim, 'clef':clef_dim, 'ori':feat_dim, 'concat':feat_dim+esm_dim, 'clefcat':clef_dim+esm_dim}
output = pd.DataFrame({})
break
for mode in test_candidate:
break
tmp_train_config = {'feature':train_data_config[mode], 'label':train_data_config[label_tag]}
Dataset = Potein_rep_datasets(tmp_train_config, label_tag = label_tag)
test_data_config
Dataset.train_range = train_range
train_range
Dataset = Potein_rep_datasets(tmp_train_config, label_tag = label_tag)
Dataset.train_range
Dataset = split_test(0.1)
Dataset.split_test(0.1)
testset = Dataset
testset.test_range
model_config['num_embeds'] = model_param_config[mode]
model = initial_model(**model_config).to(device)
optimizer = initial_optimizer(model.parameters(), lr)
Train_config.update({'model':model, 'optimizer':optimizer,
'train_iterator':Dataset.Dataloader,
'test_iterator':testset.Dataloader,
'device':device})
tmp_result = Train(**Train_config)
mode
best_score, train_log, best_epoch = list(tmp_result.values())
best_score
best_epoch
train_log
compare_log = {}
for mode in test_candidate:
tmp_train_config = {'feature':train_data_config[mode], 'label':train_data_config[label_tag]}
Dataset = Potein_rep_datasets(tmp_train_config, label_tag = label_tag)
if train_range:
Dataset.train_range = train_range
if test_data_config:
tmp_test_config = {'feature':test_data_config[mode], 'label':test_data_config[label_tag]}
testset = Potein_rep_datasets(tmp_test_config, label_tag = label_tag)
if test_range:
testset.test_range = test_range
else:
Dataset.split_test(0.1)
testset = Dataset
model_config['num_embeds'] = model_param_config[mode]
model = initial_model(**model_config).to(device)
optimizer = initial_optimizer(model.parameters(), lr)
Train_config.update({'model':model, 'optimizer':optimizer,
'train_iterator':Dataset.Dataloader,
'test_iterator':testset.Dataloader,
'device':device})
tmp_result = Train(**Train_config)
best_score, train_log, best_epoch = list(tmp_result.values())
tmp_log = {'best_score':best_score}
for key, value in train_log.items():
if key not in  ('Epoch','train_Loss','test_Loss'):
tmp_log[key] = value[best_epoch - 1]
print(f"{mode}:{tmp_log}\n")
compare_log[mode] = tmp_log
test_candidate
compare_log = {}
train_data_config[mode]
test_data_config
test_data_config = None
compare_log = {}
for mode in test_candidate:
tmp_train_config = {'feature':train_data_config[mode], 'label':train_data_config[label_tag]}
Dataset = Potein_rep_datasets(tmp_train_config, label_tag = label_tag)
if train_range:
Dataset.train_range = train_range
if test_data_config:
tmp_test_config = {'feature':test_data_config[mode], 'label':test_data_config[label_tag]}
testset = Potein_rep_datasets(tmp_test_config, label_tag = label_tag)
if test_range:
testset.test_range = test_range
else:
Dataset.split_test(0.1)
testset = Dataset
model_config['num_embeds'] = model_param_config[mode]
model = initial_model(**model_config).to(device)
optimizer = initial_optimizer(model.parameters(), lr)
Train_config.update({'model':model, 'optimizer':optimizer,
'train_iterator':Dataset.Dataloader,
'test_iterator':testset.Dataloader,
'device':device})
tmp_result = Train(**Train_config)
best_score, train_log, best_epoch = list(tmp_result.values())
tmp_log = {'best_score':best_score}
for key, value in train_log.items():
if key not in  ('Epoch','train_Loss','test_Loss'):
tmp_log[key] = value[best_epoch - 1]
print(f"{mode}:{tmp_log}\n")
compare_log[mode] = tmp_log
board = {'mode': list(compare_log.keys())}
board.update({x:[compare_log[i][x] for i in list(compare_log.keys())] for x in list(tmp_log.keys())})
print(pd.DataFrame(board))
output = pd.concat([output, pd.DataFrame(board)], 0)
reticulate::repl_python()
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
import torch
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
optimizer =optim.Adam(model.parameters(), lr=lr)
loss_function= ContrastiveLoss()
batch_size = 128
max_num_padding = 512
# max_num_padding = None
monitor_score = None
train_iterator = Dataset.Dataloader
test_iterator = Dataset.Dataloader
monitor = Metrics(metrics_dict = {})
num_epoch = 10
monitor_score = None
import numpy as np
import pickle
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
import random
import os
import pandas as pd
from Data_utils import Potein_rep_datasets
from Module import test_dnn, BCELoss, ContrastiveLoss
from utils import Metrics, Train
from Metrics_method import compute_acc, compute_f1, compute_mcc
data_path_config = {'esm_feature':"../Benchmark-partB/DeepSecE_test_esm(512)",
"B_feature":"../Benchmark-partB/BastionX/CLEF_feature/DeepSecE_train_bastionX_ori"}
Dataset = Potein_rep_datasets(data_path_config)
import torch
from CLEF import clef_for_publish
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = clef_for_publish(1280, 5).to(device)
import torch.optim as optim
import os
lr = 0.00002
optimizer =optim.Adam(model.parameters(), lr=lr)
loss_function= ContrastiveLoss()
batch_size = 128
max_num_padding = 512
monitor_score = None
train_iterator = Dataset.Dataloader
test_iterator = Dataset.Dataloader
monitor = Metrics(metrics_dict = {})
num_epoch = 10
monitor_score = None
from utils import test_clef, train_epoch_clef, trainer
train_config = {
'train_iterator':Dataset.Dataloader,
'loss_function':loss_function,
'optimizer':optimizer,
'batch_size':128,
'max_num_padding':512,
'monitor':monitor,
'device':device
}
test_config = {}
self = trainer(train_epoch = train_epoch_clef)
num_epoch = 5
self.Train(model, num_epoch=num_epoch, train_config=train_config, test_config={},early_stop_patience=30)
num_epoch = 3
self.Train(model, num_epoch=num_epoch, train_config=train_config, test_config={},early_stop_patience=30)
