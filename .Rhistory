import plotly.express as px
plot_df = pd.read_csv('D:/Dash/play_ground/temp_feat_umap.csv')
View(plot_df)
input_feature = 'D:/Dash/play_ground/temp_seq_rep'
bg_feature_path = 'D:/Dash/database/'
output_df = 'D:/Dash/play_ground/temp_feat_umap.csv'
draw_umap_feature_plot(input_feature, bg_feature_path, output_df)
plot_df = pd.read_csv('D:/Dash/play_ground/temp_feat_umap.csv')
View(plot_df)
fig = px.scatter_3d(
plot_df[mask],
x="0",
y="1",
z="2",
color="labels",
text="IDs",
)
fig = px.scatter_3d(
plot_df,
x="0",
y="1",
z="2",
color="labels",
text="IDs",
)
fig.update_traces(textfont=dict(size=12))
fig.show()
fig.update_traces(textfont=dict(size=0))
fig = px.scatter_3d(
plot_df,
x="0",
y="1",
z="2",
color="labels",
text="IDs",
)
fig.update_traces(textfont=dict(size=0.01))
fig.update_traces(textfont=dict(size=1))
fig.show()
plot_df = pd.read_csv('D:/Dash/play_ground/temp_feat_umap.csv')
test_fig = px.scatter_3d(
plot_df,
x="0",
y="1",
z="2",
color="labels",
text="IDs",
)
test_fig.update_traces(textfont=dict(size=1))
Job_lists_path = 'D:/Dash/Job_Store/'
job_lists = sorted(os.listdir(Job_lists_path), reverse=True)
job = job_lists[0]
path = os.path.join(Job_lists_path, job)
os.path.exists(os.path.join(path, './temp_feat_map.csv'))
path
df = pd.read_csv(os.path.join(path, './temp_feat_map.csv'))
df = pd.read_csv(os.path.join(path, './temp_feat_umap.csv'))
plot_df = pd.read_csv(os.path.join(path, './temp_feat_umap.csv'))
fig = px.scatter_3d(
plot_df,
x="0",
y="1",
z="2",
color="labels",
text="IDs",
)
fig.update_traces(textfont=dict(size=1.5))
umap_plot = dcc.Graph(figure = fig)
reticulate::repl_python()
import os
import requests
import time
import esm
import numpy as np
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO
import torch
import random
seq = 'MQQSHQAGYANAADRESGIPAAVLDGIKAVAKEKNATLMFRLVNPHSTSLIAEGVATKGLGVHAKSSDWGLQAGYIPVNPNLSKLFGRAPEVIARADNDVNSSLAHGHTAVDLTLSKERLDYLRQAGLVTGMADGVVASNHAGYEQFEFRVKETSDGRYAVQYRRKGGDDFEAVKVIGNAAGIPLTADIDMFAIMPHLSNFRDSARSSVTSGDSVTDYLARTRRAASEATGGLDRERIDLLWKIARAGARSAVGTEARRQFRYDGDMNIGVITDFELEVRNALNRRAHAVGAQDVVQHGTEQNNPFPEADEKIFVVSATGESQMLTRGQLKEYIGQQRGEGYVFYENRAYGVAGKSLFDDGLGAAPGVPGGRSKSSPDVLETVPASPGLRRPSLGAVERQDSGYDSLDGVGSRSFSLGEVSDMAAVEAAELEMTRQVLHAGARQDDAEPGVSGASAHWGQRALQGAQAVAAAQRLVHAIALMTQFGRAGSTNTPQEAASLSAAVFGLGEASSAVAETVSGFFRGSSRWAGGFGVAGGAMALGGGIAAAVGAGMSLTDDAPAGQKAAAGAEIALQLTGGTVELASSIALALAAARGVTSGLQVAGASAGAAAGALAAALSPMEIYGLVQQSHYADQLDKLAQESSAYGYEGDALLAQLYRDKTAAEGAVAGVSAVLSTVGAAVSIAAAASVVGAPVAVVTSLLTGALNGILRGVQQPIIEKLANDYARKIDELGGPQAYFEKNLQARHEQLANSDGLRKMLADLQAGWNASSVIGVQTTEISKSALELAAITGNADNLKSADVFVDRFIQGERVAGQPVVLDVAAGGIDIASRKGERPALTFITPLAAPGEEQRRRTKTGKSEFTTFVEIVGKQDRWRIRDGAADTTIDLAKVVSQLVDANGVLKHSIKLEVIGGDGDDVVLANASRIHYDGGAGTNTVSYAALGRQDSITVSADGERFNVRKQLNNANVYREGVATQKTAYGKRTENVQYRHVELARVGQLVEVDTLEHVQHIIGGAGNDSITGNAHDNFLAGGAGDDRLDGGAGNDTLVGGEGHNTVVGGAGDDVFLQDLGVWSNQLDGGAGVDTVKYNVHQPSEERLERMGDTGIHADLQKGTVEKWPALNLFSVDHVKNIENLHGSSLNDSIAGDDRDNELWGDDGNDTIHGRGGDDILRGGLGLDTLYGEDGNDIFLQDDETVSDDIDGGAGLDTVDYSAMIHAGKIVAPHEYGFGIEADLSEGWVRKAARRGMDYYDSVRSVENVIGTSMKDVLIGDAQANTLMGQGGDDTVRGGDGDDLLFGGDGNDMLYGDAGNDTLYGGLGDDTLEGGAGNDWFGQTPAREHDVLRGGAGVDTVDYSQAGAHAGVATGRIGLGILADLGAGRVDKLGEAGSSAYDTVSGIENVVGTELADRITGDAQANVLRGAGGADVLAGGEGDDVLLGGDGDDQLSGDAGRDRLYGEAGDDWFFQDAANAGNLLDGGDGNDTVDFSGPGRGLDAGAKGVFLSLGKGFASLMDEPETSNVLRHIENAVGSVRDDVLIGDAGANVLNGLAGNDVLSGGAGDDVLLGDEGSDLLSGDAGNDDLFGGQGDDTYLFGAGYGHDTIYESGGGHDTIRINAGADQLWFARQGNDLEIRILGTDDALTVHDWYRDADHRVEAIHAANQAIDPAGIEKLVEAMAQYPDPGAAAAAPPAARVPDTLMQSLAVNWR'
maxlen = 128
maxlen = 196
model_path = 'D:/Dash/database/esm2_t33_650M_UR50D.pt'
model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_path)
model_path = 'D:/Dash/database/esm2_t33_650M_UR50D.pt'
model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_path)
batch_converter = alphabet.get_batch_converter()
model.eval()
last_layer = len(model.layers)
seq_rep = {}
token_rep = {}
sequence = seq[:maxlen]
data = [
("protein1", str(sequence))
]
batch_labels, batch_strs, batch_tokens = batch_converter(data)
with torch.no_grad():
results = model(batch_tokens, repr_layers=[last_layer], return_contacts=True)
token_representations = results["representations"][last_layer]
tokens_len = token_representations.shape[1]
sequence_representations = token_representations[0, 1 : tokens_len - 1].mean(0)
token_representations = token_representations.squeeze().numpy().astype(np.float16)
sequence_representations = sequence_representations.numpy().astype(np.float16)
print(token_representations.shape)
library(ggplot2)
library(ggplot2)
PCA_plot = function(input_file,meta_file,scale = F){
meta = read.delim(meta_file)
#meta = data.frame(cbind(file = row.names(meta), group = meta[,1], name = meta[,2]))
all_groups = meta$group
names(all_groups) = meta$name
data = readxl::read_excel(input_file)
features = t(data[, 2:ncol(data)])
sele_col = c()
for (i in 1:ncol(features)) {
discrim = sum(features[,i] == 0)
if(discrim == nrow(features)){
next
}
sele_col  = c(sele_col, i)
}
features = features[, sele_col]
if(scale){
features = scale(features)
}
pca_result <- prcomp(features)
pca_data <- as.data.frame(pca_result$x)
pca_data$group <- all_groups[row.names(pca_data)]
ggplot(data = pca_data, mapping = aes(x = PC1, y = PC2, color = group)) +
geom_point() +
theme_bw() +
ylim(c(-5, 5))
}
import os
reticulate::repl_python()
import os
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO
from Bio.Seq import Seq
import random
import pickle
import torch
import numpy as np
import umap
import matplotlib.pyplot as plt
import pandas as pd
import shutil
import numpy as np
import pickle
import torch
def load_feature_from_local(feature_path):
'''
load feature dict from local path (Using pickle.load() or torch.load())
the dictionary is like:
{
Protein_ID : feature_array [a 1D numpy array]
}
'''
# Try pickle.load() function
try:
with open(feature_path, 'rb') as f:
obj = pickle.load(f)
print("File is loaded using pickle.load()")
return obj
except (pickle.UnpicklingError, EOFError):
pass
# Try torch.load() function
try:
obj = torch.load(feature_path)
print("File is loaded using torch.load()")
return obj
except (torch.serialization.UnsupportedPackageTypeError, RuntimeError):
pass
print("Unable to load file.")
return None
def assign_labels(sample_ids, tag_label_mapping):
'''
iterate a list of sample ids, generate a ID-label dict by the given tag_label mapping dict
label is assigned by discriminating whether the identifier string tag appears in ID string
the ID not mapping any tag will be assigned 0 by default
the dictionary is like:
{
Protein_ID : label [integer]
}
'''
labeled_dict = {}
for sample_id in sample_ids:
apperance = np.array([x in sample_id for x in tag_label_mapping])
if apperance.sum() > 0: # discriminate tag appear in sample id
label = list(tag_label_mapping.values())[
np.where(apperance == 1)[0][-1] # in case more than one tag appear in ID, only the last was consider
]
else:
label = 0
labeled_dict[sample_id] = label
return labeled_dict
def Euclidean_Distance(features):
'''
input an 2D array (num_sample * num_feature) and compute distances
return a distance matrix array (num_sample * num_sample)
'''
distances = np.sqrt(np.sum((features[:, np.newaxis] - features) ** 2, axis=-1))
return distances
def Cosine_Similarity(features):
'''
input an 2D array (num_sample * num_feature) and compute distances
return a distance matrix array (num_sample * num_sample)
'''
from sklearn.metrics.pairwise import cosine_similarity
distances = cosine_similarity(features)
return distances
def KNN_feature(features_dict, distance_method, labeled_dict = None, K = 8):
'''
K-nearest neighbours method used to find similar sample in feature space
distance will be compute using Euclidean Distance or Cosine Similarity
Optional labeled_dict can be provided to count the labels
return a dict of nearest_neighbors
the dictionary is like:
{
Protein_ID : {
'neighbours':nearest_samples [a list of nearest sample ids],
'labels':label_counts[a dict of nearest label count]
}
}
labeled_dict = assign_labels(sample_ids, tag_label_mapping)
features_dict = x
distance_method = Euclidean_Distance
'''
samples = list(features_dict.keys())
features = np.array(list(features_dict.values()))
distances = distance_method(features)
nearest_neighbors = {}
for i, sample in enumerate(samples):
distances[i, i] = np.inf
nearest_indices = np.argsort(distances[i])[:K]
nearest_samples = [samples[j] for j in nearest_indices]
if not labeled_dict:
nearest_sample_labels = [labeled_dict[sample] for sample in nearest_samples]
counts = np.unique(nearest_sample_labels, return_counts=True)
label_counts = dict(zip(counts[0], counts[1]))
else:
label_counts = None
nearest_neighbors[sample] = {'neighbours':nearest_samples, 'labels':label_counts}
return nearest_neighbors
def Feature_concatenate(*feature_dicts):
'''
merge / combine input features by concatenation
the input dictionary is like:
{
Protein_ID : feature_array [a 1D numpy array]
}
'''
merged_dict = {}
max_dim = 0
for d in feature_dicts:
for key, value in d.items():
if key in merged_dict:
merged_dict[key] = np.concatenate([merged_dict[key], value], 0)
else:
merged_dict[key] = value
max_dim = merged_dict[key].shape[0]  if max_dim < merged_dict[key].shape[0] else max_dim
output_dict = {}
for key, value in merged_dict.items():
if value.shape[0] == max_dim:  # only merged value was output
output_dict[key] = value
return output
def Set_Seed(seed):
'''
initialize random seeds.
'''
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True
reticulate::repl_python()
import sys
sys.path.append('D:/R code/torch/')
import torch
import torch.nn as nn
import torch.nn.functional as F
from protein_loader import Foldseek_ego_feature_datasets
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from metrics import accuracy, find_best_accuracy, find_false_prediction_test
import torch.optim as optim
import numpy as np
import pandas as pd
class ego_graph_conv(nn.Module):
def __init__(self, input_features , input_embedding, nodes_concat = True, num_hidden = 256, drop_out=0.45):
super().__init__()
self.node_fc = nn.Linear(320, num_hidden, bias = False)
self.edge_fc = nn.Linear(5, 16, bias = False)
self.edge_fc = nn.Sequential(nn.Linear(5, 16, bias = False), nn.ReLU(), nn.Linear(16, 1))
input_hidden = input_embedding
self.fc1 = nn.Linear(input_hidden, num_hidden)
self.ln1 = nn.LayerNorm(num_hidden)
self.fc2 = nn.Linear(num_hidden, 2 * num_hidden)
self.fc3 = nn.Linear(2 * num_hidden, num_hidden)
self.out = nn.Linear(num_hidden, 1)
self.drop = nn.Dropout(drop_out)
self.num_hidden = num_hidden
self.nodes_concat = nodes_concat
def forward(self, input_data):
x = input_data['seq_input']
n = input_data["valid_lens"].to("cpu")
ego = input_data['ego']
btz = x.shape[0]
if self.nodes_concat:
gate = x[:, :, :5]
gate = torch.sigmoid(self.edge_fc(gate))
nodes_feat = x[:, :, 5:]
nodes_feat = self.node_fc(nodes_feat)
nodes_feat = gate * nodes_feat
x = self.fc1(ego)
x += nodes_feat.mean(-2)
else:
x = self.fc1(ego)
x = self.ln1(x)
x = self.drop(self.fc3(F.relu(self.fc2(x))))
out = torch.sigmoid(self.out(x)).squeeze(-1)
return out
input_file = "./cluster_kNN_T3/exp/machinefeature/deepsec_train_graph_feat_dataset"
reticulate::repl_python()
import torch
import subprocess as sp
import torch
import os
import shlex
import argparse
import tempfile
import typing as T
from Bio import SeqIO, SeqRecord, Seq
from .utils import log
from datetime import datetime
curr_time = f"[{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}] "
log_string = f"{curr_time if timestamped else ''}{m}"
timestamped=True
log_string = f"{curr_time if timestamped else ''}{m}"
def log(m, file=None, timestamped=True, print_also=False):
curr_time = f"[{datetime.now().strftime('%Y-%m-%d-%H:%M:%S')}] "
log_string = f"{curr_time if timestamped else ''}{m}"
if file is None:
print(log_string)
else:
print(log_string, file=file)
if print_also:
print(log_string)
file.flush()
fold_vocab = {
"D": 0,
"P": 1,
"V": 2,
"Q": 3,
"A": 4,
"W": 5,
"K": 6,
"E": 7,
"I": 8,
"T": 9,
"L": 10,
"F": 11,
"G": 12,
"S": 13,
"M": 14,
"H": 15,
"C": 16,
"R": 17,
"Y": 18,
"N": 19,
"X": 20,
}
tempfile.TemporaryDirectory()
reticulate::repl_python()
reticulate::repl_python()
import os
import pandas as pd
import numpy as np
import pickle
from Bio.SeqRecord import SeqRecord
from Bio import SeqIO
from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel
import transformers
print("transformers:", transformers.__version__)
quit
setwd("D:/ubuntu_temp_file/github-submit/Demo")
reticulate::repl_python()
import os
os.chdir("..")
from src.Feature_transform import generate_msa_transformer_feat
quit
setwd("D:/R code/TEST/MSA")
reticulate::repl_python()
os.mkdir("./Demo_MSA")
input_dir = "./combined_nored_prots_pdb_validate/"
MSAs = os.listdir(input_dir)
import random
import shutil
input_dir = "./combined_nored_prots_pdb_validate/"
MSAs = os.listdir(input_dir)
import random
import shutil
for x in random.sample(MSAs, 10):
path = os.path.join(input_dir, x)
tgt = os.path.join("./Demo_MSA", x)
shutil.copy(path, tgt)
quit
setwd("D:/ubuntu_temp_file/github-submit/Demo")
os.chdir("..")
reticulate::repl_python()
import os
os.chdir("..")
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
import os
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
config = {
'input_alignments_dir' : "./Demo/Demo_MSA/", # input dirname of MSA
'maxlen' : 1024,
'output_file' : "./Demo/Demo_MSA_feat", # output feature path
'remapping_fasta' : None,
'clust_pool' : True,  # average pooling across the cluster (first) dimension
'res_pool' : True,  # average pooling across the residue (second)  dimension
'maxmsa' : 8,
'suffix':'fasta'
}
generate_msa_transformer_feat(**config)
reticulate::repl_python()
import os
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
config = {
'input_alignments_dir' : "./Demo/Demo_MSA/", # input dirname of MSA
'maxlen' : 1024,
'output_file' : "./Demo/Demo_MSA_feat", # output feature path
'remapping_fasta' : None,
'clust_pool' : True,  # average pooling across the cluster (first) dimension
'res_pool' : True,  # average pooling across the residue (second)  dimension
'maxmsa' : 8,
'suffix':'fasta'
}
generate_msa_transformer_feat(**config)
aa_dict = {amino_acid: i for i, amino_acid in enumerate("ACDEFGHIKLMNPQRSTVWYX")}
model, alphabet = esm.pretrained.load_model_and_alphabet_local(pretrained_model_params)
import torch
import esm
aa_dict = {amino_acid: i for i, amino_acid in enumerate("ACDEFGHIKLMNPQRSTVWYX")}
model, alphabet = esm.pretrained.load_model_and_alphabet_local(pretrained_model_params)
pretrained_model_params = '../pretained_model/esm_msa1b_t12_100M_UR50S.pt'
suffix = 'fasta'
model, alphabet = esm.pretrained.load_model_and_alphabet_local(pretrained_model_params)
print(f"Skip loading local pre-trained ESM2 model from {pretrained_model_params}.\nTry to download msa-transformer from fair-esm")
input_embedding_net, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()
input_embedding_net, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()
print(f"Skip loading local pre-trained ESM2 model from {pretrained_model_params}.\nTry to download msa-transformer from fair-esm")
input_embedding_net, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()
generate_msa_transformer_feat(**config)
torch.cuda.is_avaliable()
device = "cuda:0" if torch.cuda.is_available() else "cpu"
reticulate::repl_python()
import os
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
config = {
'input_alignments_dir' : "./Demo/Demo_MSA/", # input dirname of MSA
'maxlen' : 1024,
'output_file' : "./Demo/Demo_MSA_feat", # output feature path
'remapping_fasta' : None,
'clust_pool' : True,  # average pooling across the cluster (first) dimension
'res_pool' : True,  # average pooling across the residue (second)  dimension
'maxmsa' : 8,
'suffix':'fasta'
}
generate_msa_transformer_feat(**config)
import os
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
config = {
'input_alignments_dir' : "./Demo/Demo_MSA/", # input dirname of MSA
'maxlen' : 1024,
'output_file' : "./Demo/Demo_MSA_feat", # output feature path
'remapping_fasta' : None,
'clust_pool' : True,  # average pooling across the cluster (first) dimension
'res_pool' : True,  # average pooling across the residue (second)  dimension
'maxmsa' : 8,
'suffix':'fasta'
}
generate_msa_transformer_feat(**config)
reticulate::repl_python()
import os
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
config = {
'input_alignments_dir' : "./Demo/Demo_MSA/", # input dirname of MSA
'maxlen' : 1024,
'output_file' : "./Demo/Demo_MSA_feat", # output feature path
'remapping_fasta' : None,
'clust_pool' : True,  # average pooling across the cluster (first) dimension
'res_pool' : True,  # average pooling across the residue (second)  dimension
'maxmsa' : 8,
'suffix':'fasta'
}
generate_msa_transformer_feat(**config)
reticulate::repl_python()
import os
from src.Feature_transform import generate_msa_transformer_feat  # import feature transforming method
config = {
'input_alignments_dir' : "./Demo/Demo_MSA/", # input dirname of MSA
'maxlen' : 1024,
'output_file' : "./Demo/Demo_MSA_feat", # output feature path
'remapping_fasta' : None,
'clust_pool' : True,  # average pooling across the cluster (first) dimension
'res_pool' : True,  # average pooling across the residue (second)  dimension
'maxmsa' : 8,
'suffix':'fasta'
}
generate_msa_transformer_feat(**config)
